# -*- coding: utf-8 -*-
"""Pre-train BERT Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lPazDXou0ZFol5Y2HpDJcFMiVuHkATzZ
"""

import json
import tensorflow as tf
import pandas as pd
import time
import os
import numpy as np
from datetime import datetime
import requests
from requests_oauthlib import OAuth1
from pathlib import Path
import re
import urllib
from dateutil import parser

from google.colab import drive
drive.mount("/content/gdrive/")
path = "./gdrive/Shareddrives/FinTech21/"

"""# Twitter Prediction"""

pip install transformers

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler
from torch import cuda
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer,BertModel,BertConfig,BertForTokenClassification,BertForSequenceClassification,get_linear_schedule_with_warmup
import torch.nn.functional as F
from tqdm import tqdm
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report 
import random

import transformers
from transformers import BertTokenizer, TFBertForSequenceClassification,BertForTokenClassification 
from transformers import InputExample, InputFeatures
from sklearn.model_selection import train_test_split
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

params = list(model.named_parameters())

print('The BERT model has {:} different named parameters.\n'.format(len(params)))

print('==== Embedding Layer ====\n')

for p in params[0:5]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

print('\n==== First Transformer ====\n')

for p in params[5:21]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

print('\n==== Output Layer ====\n')

for p in params[-4:]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

model

data=pd.read_csv(path+'/tweets_with_price/tweets_6h.csv')

data['time']=data['time'].apply(lambda timestamp: parser.parse(timestamp))
data['text']=[str(t) for t in data.text]
data['text']=data['text'].map(lambda x: x.lower())
data["label"]=[1 if l==1 else 0 for l in data.up_down] # 改成 1、0

# train 1-7 /val=8、9 /test=10-11.5
train=data.loc[data['time'].dt.month <8]
val=data.loc[(data['time'].dt.month >=8)&(data['time'].dt.month <10)]
test=data.loc[data['time'].dt.month >=10]
print(train.shape)
print(val.shape)
print(test.shape)

def replace_layers(model, old, new):
      for n, module in model.named_children():
          if len(list(module.children())) > 0:
              replace_layers(module, old, new)      
          if isinstance(module, old):  
              setattr(model, n, new)
      return model

class MyBertModel:
    def __init__(self,traindf,saveModel_path,learning_rate,n_class,epochs,batch_size,val_batch_size,max_len,gpu=True):
        self.n_class = n_class # Number of categories 
        self.max_len = max_len # Maximum sentence length 
        self.lr = learning_rate # Learning rate 
        self.epochs = epochs            #bert-base-uncased
        self.tokenizer = transformers.BertTokenizer.from_pretrained("bert-base-cased") # Load the word segmentation model
        # Load training dataset 
        train_x, train_y = self.load_data(train)
        val_x,  val_y = self.load_data(val)

        self.train = self.process_data(train_x, train_y)
        self.validation = self.process_data(val_x, val_y)
        self.batch_size = batch_size # Training set batch_size
        self.val_batch_size = val_batch_size
        self.saveModel_path = saveModel_path # Model storage location 
        self.gpu = gpu # Whether to use gpu

        config = BertConfig.from_pretrained("bert-base-uncased") # load bert Model configuration information

        config.num_labels = n_class # Set the output number of the classification model 
        model=BertForSequenceClassification.from_pretrained("bert-base-uncased",config=config)
        self.model=self.replace_layers(model,nn.Tanh,nn.Sigmoid())


        if self.gpu:
            seed = 42
            random.seed(seed)
            np.random.seed(seed)
            torch.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
            torch.backends.cudnn.deterministic =True
            self.device = torch.device('cuda')
        else:
            self.device = 'cpu'

    def replace_layers(self,model, old, new):
        for n, module in model.named_children():
            if len(list(module.children())) > 0:
                replace_layers(module, old, new)      
            if isinstance(module, old):  
                setattr(model, n, new)
        return model

    def encode_fn(self,text_list):
        tokenizer = self.tokenizer(
        text_list,
        padding = True,
        truncation = True,
        add_special_tokens = True,
        max_length = self.max_len,
        return_tensors='pt' # The type of return is pytorch tensor
        )
        input_ids = tokenizer['input_ids']
        token_type_ids = tokenizer['token_type_ids']
        attention_mask = tokenizer['attention_mask']
        return input_ids,token_type_ids,attention_mask

    def load_data(self,data):
        text_list = data['text'].to_list()
        labels = data['label'].to_list()
        return text_list, labels

    def process_data(self, text_list, labels):
        input_ids,token_type_ids,attention_mask = self.encode_fn(text_list)
        labels = torch.tensor(labels)
        data = torch.utils.data.TensorDataset(input_ids,token_type_ids,attention_mask,labels)
        return data

    # def split_train_val(self, data, labels):
    #     train_x, val_x, train_y, val_y = train_test_split(data,labels,test_size = 0.2,random_state = 0)
    #     return train_x, val_x, train_y, val_y


    def flat_accuracy(self, preds, labels):
  #A function for calculating accuracy scores
        pred_flat = np.argmax(preds, axis=1).flatten()
        labels_flat = labels.flatten()
        return accuracy_score(labels_flat, pred_flat)

    def train_model(self):
  # Training models 
        if self.gpu:
            self.model.cuda()
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr,eps = 1e-8)
        trainData = torch.utils.data.DataLoader(self.train, batch_size = self.batch_size, shuffle = True) # Process it into multiple batch In the form of 
        valData = torch.utils.data.DataLoader(self.validation, batch_size = self.val_batch_size, shuffle = True)
        total_steps = len(trainData) #* self.epochs

        warmup_ratio=0
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_ratio*total_steps, num_training_steps=total_steps)
        for epoch in range(self.epochs):
            self.model.train()
            total_loss, total_val_loss = 0, 0
            total_eval_accuracy = 0
            print("==================================================================================================================")
            print('epoch:' , epoch , ', step_number:' , len(trainData))
      
    # Training 
            for step,batch in enumerate(trainData):
                outputs = self.model(input_ids = batch[0].to(self.device),
                token_type_ids=batch[1].to(self.device),
                attention_mask=batch[2].to(self.device),
                labels=batch[3].to(self.device),
                  ) # Output loss and The output of each category ,softmax The prediction is the probability of the corresponding classification 
                self.model.zero_grad()
                loss, logits = outputs.loss, outputs.logits

                total_loss += loss.item()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.01)
                optimizer.step()
                scheduler.step()
                if step % 100 == 0 and step > 0: # Every time 10 Step output the training results ,flat_accuracy() Would be right logits Conduct softmax
                    logits = logits.detach().cpu().numpy()
                    label_ids = batch[3].cuda().data.cpu().numpy()
                    avg_val_accuracy = self.flat_accuracy(logits, label_ids)
                  #print('step:' , step)
                  #print(f'Accuracy: {avg_val_accuracy:.4f}')
                  #print('\n')
            # Every epoch end , Just use validation Data set evaluation model once 
                    self.model.eval()
                    print('testing ....')

            for i, batch in enumerate(valData):
                with torch.no_grad():
                    loss, logits = self.model(input_ids=batch[0].to(self.device),
                  token_type_ids=batch[1].to(self.device),
                  attention_mask=batch[2].to(self.device),
                  labels=batch[3].to(self.device),return_dict=False
                    )

                    total_val_loss += loss.item()
                    logits = logits.detach().cpu().numpy()
                    label_ids = batch[3].cuda().data.cpu().numpy()
                    total_eval_accuracy += self.flat_accuracy(logits, label_ids)
            avg_train_loss = total_loss / len(trainData)
            avg_val_loss = total_val_loss / len(valData)
            avg_val_accuracy = total_eval_accuracy / len(valData)
            print(f'Train loss : {avg_train_loss}')
            print(f'Validation loss: {avg_val_loss}')
            print(f'Accuracy: {avg_val_accuracy:.4f}')
            print('\n')
            self.save_model(self.saveModel_path + '-' + str(epoch))

    def save_model(self , path):
  # Save the segmentation model and classification model 
        self.model.save_pretrained(path)
        self.tokenizer.save_pretrained(path)


    def load_model(self,path):
  # Loading segmentation model and classification model 
        tokenizer = BertTokenizer.from_pretrained(path)
        model = BertForSequenceClassification.from_pretrained(path)
        return tokenizer,model


    def eval_model(self,Tokenizer, model,text_list,y_true):
  # Recall rate of output model 、 Accuracy rate 、f1-score
        preds,logits = self.predict_batch(Tokenizer, model, text_list)
        print(classification_report(y_true,preds))
        return preds,logits

    def predict_batch(self, Tokenizer, model, text_list):
        tokenizer = Tokenizer(
        text_list,
        padding = True,
        truncation = True,
        max_length = self.max_len,
        return_tensors='pt' # The type of return is pytorch tensor
        )
        input_ids = tokenizer['input_ids']
        token_type_ids = tokenizer['token_type_ids']
        attention_mask = tokenizer['attention_mask']
        pred_data = torch.utils.data.TensorDataset(input_ids,token_type_ids,attention_mask)
        pred_dataloader = DataLoader(pred_data, batch_size=self.batch_size, shuffle=False)
        model = model.to(self.device)
        model.eval()
        preds = []
        logitss = []
        for i, batch in enumerate(pred_dataloader):
            with torch.no_grad():
                outputs = self.model(input_ids=batch[0].to(self.device),
                token_type_ids=batch[1].to(self.device),
                attention_mask=batch[2].to(self.device),return_dict=False
                )
      
            logits = outputs[0]
            #print(logits)
            l=[F.softmax(i) for i in logits] 
            logitss.append(l)
            logits = logits.detach().cpu().numpy()
       
            preds += list(np.argmax(logits, axis=1))

        return preds,logitss

import gc

gc.collect()

torch.cuda.empty_cache()

model, li = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 5, output_loading_info=True) # as we have 5 classes
print(li)

epoch = 10
#pretrained_path = "/content/drive/Shareddrives/xiximayou/pretrained/bert-base-uncased"
dataset_path = "/content/"
save_path = "/content/"
save_model_path = os.path.join(dataset_path, "pt/bert-base-bert-base-uncased")
model_name = "bert_tweet"


myBertModel = MyBertModel(
traindf=train,
saveModel_path = os.path.join(save_model_path, model_name),
learning_rate = 5e-5, #：5e-5, 3e-5, 2e-5
n_class = 2,
epochs = epoch,
batch_size = 16,
val_batch_size =16,
max_len = 512,
gpu = True
)

myBertModel.train_model()
Tokenizer, model = myBertModel.load_model(myBertModel.saveModel_path + '-'+ str(epoch-1))

text_list, y_true = myBertModel.load_data(train)
pred,log=myBertModel.eval_model(Tokenizer, model,text_list,y_true)

text_list, y_true = myBertModel.load_data(val)
pred2,log=myBertModel.eval_model(Tokenizer, model,text_list,y_true)

text_list, y_true = myBertModel.load_data(test)
pred3,log=myBertModel.eval_model(Tokenizer, model,text_list,y_true)